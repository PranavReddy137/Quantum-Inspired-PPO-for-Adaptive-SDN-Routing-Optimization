{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2ReIK9udIQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkTopology:\n",
        "    def __init__(self, num_nodes, connectivity_prob=0.3, seed=None):\n",
        "\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.num_nodes = num_nodes\n",
        "        self.G = nx.Graph()\n",
        "\n",
        "        # Add nodes with attributes\n",
        "        for i in range(num_nodes):\n",
        "            self.G.add_node(i,\n",
        "                           bandwidth=random.randint(10, 100),  # Mbps\n",
        "                           latency=random.uniform(1, 10),      # ms\n",
        "                           queue_size=random.randint(50, 200), # packets\n",
        "                           processing_power=random.uniform(0.5, 2.0))  # processing capability\n",
        "\n",
        "        # Create edges with probability connectivity_prob\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(i+1, num_nodes):\n",
        "                if random.random() < connectivity_prob:\n",
        "                    # Add edge with network attributes\n",
        "                    self.G.add_edge(i, j,\n",
        "                                  bandwidth=min(self.G.nodes[i]['bandwidth'],\n",
        "                                               self.G.nodes[j]['bandwidth']),\n",
        "                                  latency=random.uniform(5, 20),        # ms\n",
        "                                  packet_loss=random.uniform(0, 0.05),  # 0-5% packet loss\n",
        "                                  throughput=random.randint(100, 1000), # Mbps\n",
        "                                  utilization=random.uniform(0.1, 0.5),  # 10-50% utilization\n",
        "                                  congestion=random.uniform(0, 0.3))    # congestion level\n",
        "\n",
        "        # Ensure the graph is connected\n",
        "        if not nx.is_connected(self.G):\n",
        "            components = list(nx.connected_components(self.G))\n",
        "            for i in range(1, len(components)):\n",
        "                node1 = random.choice(list(components[0]))\n",
        "                node2 = random.choice(list(components[i]))\n",
        "                self.G.add_edge(node1, node2,\n",
        "                              bandwidth=min(self.G.nodes[node1]['bandwidth'],\n",
        "                                           self.G.nodes[node2]['bandwidth']),\n",
        "                              latency=random.uniform(5, 20),\n",
        "                              packet_loss=random.uniform(0, 0.05),\n",
        "                              throughput=random.randint(100, 1000),\n",
        "                              utilization=random.uniform(0.1, 0.5),\n",
        "                              congestion=random.uniform(0, 0.3))\n",
        "\n",
        "        # Initialize traffic matrix and routing table\n",
        "        self.initialize_traffic_matrix()\n",
        "        self.initialize_routing_table()\n",
        "\n",
        "        # Current network state\n",
        "        self.current_time = 0\n",
        "        self.packet_queue = defaultdict(list)  # {node_id: [packets]}\n",
        "        self.delivered_packets = 0\n",
        "        self.dropped_packets = 0\n",
        "        self.total_delay = 0\n",
        "\n",
        "    def initialize_traffic_matrix(self):\n",
        "        \"\"\"Initialize a traffic matrix between all pairs of nodes\"\"\"\n",
        "        self.traffic_matrix = np.zeros((self.num_nodes, self.num_nodes))\n",
        "        for i in range(self.num_nodes):\n",
        "            for j in range(self.num_nodes):\n",
        "                if i != j:\n",
        "                    # Random traffic demand between nodes (packets per second)\n",
        "                    self.traffic_matrix[i, j] = random.randint(5, 50)\n",
        "\n",
        "    def initialize_routing_table(self):\n",
        "        \"\"\"Initialize routing tables using shortest path algorithm\"\"\"\n",
        "        self.routing_table = {}\n",
        "        # Calculate shortest paths based on latency\n",
        "        for i in range(self.num_nodes):\n",
        "            self.routing_table[i] = {}\n",
        "            for j in range(self.num_nodes):\n",
        "                if i != j:\n",
        "                    try:\n",
        "                        # Use latency as the weight for path calculation\n",
        "                        path = nx.shortest_path(self.G, source=i, target=j,\n",
        "                                               weight=lambda u, v, d: d['latency'])\n",
        "                        self.routing_table[i][j] = path\n",
        "                    except nx.NetworkXNoPath:\n",
        "                        self.routing_table[i][j] = None\n",
        "\n",
        "    def update_routing_table(self, new_routing_table):\n",
        "        \"\"\"Update the routing table (will be used by the agents)\"\"\"\n",
        "        self.routing_table = new_routing_table\n",
        "\n",
        "    def generate_packet(self, source, destination, size=1024):\n",
        "        \"\"\"Generate a packet with given parameters\"\"\"\n",
        "        return {\n",
        "            'id': f\"pkt_{source}_{destination}_{self.current_time}_{random.randint(0, 1000)}\",\n",
        "            'source': source,\n",
        "            'destination': destination,\n",
        "            'size': size,  # bytes\n",
        "            'creation_time': self.current_time,\n",
        "            'current_node': source,\n",
        "            'next_hop': None,\n",
        "            'hops': 0,\n",
        "            'delay': 0\n",
        "        }\n",
        "\n",
        "    def step(self, time_step=1):\n",
        "        \"\"\"Simulate network for one time step\"\"\"\n",
        "        self.current_time += time_step\n",
        "\n",
        "        # Generate new packets according to traffic matrix\n",
        "        for source in range(self.num_nodes):\n",
        "            for destination in range(self.num_nodes):\n",
        "                if source != destination:\n",
        "                    # Number of packets to generate based on traffic rate\n",
        "                    num_packets = np.random.poisson(self.traffic_matrix[source, destination] * time_step / 60)\n",
        "                    for _ in range(num_packets):\n",
        "                        packet = self.generate_packet(source, destination)\n",
        "                        self.packet_queue[source].append(packet)\n",
        "\n",
        "        # Process packets at each node\n",
        "        for node in range(self.num_nodes):\n",
        "            # Get processing capacity\n",
        "            processing_power = self.G.nodes[node]['processing_power']\n",
        "            queue_size = self.G.nodes[node]['queue_size']\n",
        "\n",
        "            # Check if queue is overflowing\n",
        "            if len(self.packet_queue[node]) > queue_size:\n",
        "                # Drop packets that exceed queue size\n",
        "                dropped = len(self.packet_queue[node]) - queue_size\n",
        "                self.dropped_packets += dropped\n",
        "                self.packet_queue[node] = self.packet_queue[node][:queue_size]\n",
        "\n",
        "            # Process packets based on node's processing power\n",
        "            packets_to_process = min(int(processing_power * 10), len(self.packet_queue[node]))\n",
        "\n",
        "            for _ in range(packets_to_process):\n",
        "                if not self.packet_queue[node]:\n",
        "                    break\n",
        "\n",
        "                packet = self.packet_queue[node].pop(0)\n",
        "\n",
        "                # If packet reached destination\n",
        "                if packet['current_node'] == packet['destination']:\n",
        "                    self.delivered_packets += 1\n",
        "                    delay = self.current_time - packet['creation_time']\n",
        "                    self.total_delay += delay\n",
        "                    continue\n",
        "\n",
        "                # Find next hop using routing table\n",
        "                route = self.routing_table[packet['current_node']][packet['destination']]\n",
        "                if not route:\n",
        "                    # No route to destination, drop packet\n",
        "                    self.dropped_packets += 1\n",
        "                    continue\n",
        "\n",
        "                if len(route) > 1:\n",
        "                    next_hop = route[1]  # Next node in the path\n",
        "\n",
        "                    # Check link conditions\n",
        "                    edge_data = self.G.get_edge_data(packet['current_node'], next_hop)\n",
        "\n",
        "                    # Check for packet loss\n",
        "                    if random.random() < edge_data['packet_loss']:\n",
        "                        self.dropped_packets += 1\n",
        "                        continue\n",
        "\n",
        "                    # Update packet information\n",
        "                    packet['current_node'] = next_hop\n",
        "                    packet['hops'] += 1\n",
        "                    packet['delay'] += edge_data['latency']\n",
        "\n",
        "                    # Add to next node's queue\n",
        "                    self.packet_queue[next_hop].append(packet)\n",
        "\n",
        "                    # Update link utilization\n",
        "                    edge_data['utilization'] += packet['size'] / (edge_data['bandwidth'] * 1024 * 1024)\n",
        "                    if edge_data['utilization'] > 1:\n",
        "                        edge_data['utilization'] = 1\n",
        "                        edge_data['congestion'] += 0.1\n",
        "                        if edge_data['congestion'] > 1:\n",
        "                            edge_data['congestion'] = 1\n",
        "\n",
        "        # Decay link utilization and congestion over time\n",
        "        for u, v, data in self.G.edges(data=True):\n",
        "            data['utilization'] *= 0.95  # 5% decay per time step\n",
        "            data['congestion'] *= 0.9    # 10% decay per time step\n",
        "\n",
        "        # Return network performance metrics\n",
        "        return self.get_performance_metrics()\n",
        "\n",
        "    def get_performance_metrics(self):\n",
        "        \"\"\"Calculate and return network performance metrics\"\"\"\n",
        "        # Calculate average metrics across the network\n",
        "        avg_latency = 0\n",
        "        avg_packet_loss = 0\n",
        "        avg_throughput = 0\n",
        "        avg_utilization = 0\n",
        "        avg_congestion = 0\n",
        "\n",
        "        if self.G.number_of_edges() > 0:\n",
        "            for u, v, data in self.G.edges(data=True):\n",
        "                avg_latency += data['latency']\n",
        "                avg_packet_loss += data['packet_loss']\n",
        "                avg_throughput += data['throughput']\n",
        "                avg_utilization += data['utilization']\n",
        "                avg_congestion += data['congestion']\n",
        "\n",
        "            avg_latency /= self.G.number_of_edges()\n",
        "            avg_packet_loss /= self.G.number_of_edges()\n",
        "            avg_throughput /= self.G.number_of_edges()\n",
        "            avg_utilization /= self.G.number_of_edges()\n",
        "            avg_congestion /= self.G.number_of_edges()\n",
        "\n",
        "        # Calculate end-to-end metrics\n",
        "        packet_delivery_ratio = 0\n",
        "        avg_end_to_end_delay = 0\n",
        "\n",
        "        total_packets = max(1, self.delivered_packets + self.dropped_packets)  # Avoid division by zero\n",
        "        packet_delivery_ratio = self.delivered_packets / total_packets\n",
        "\n",
        "        if self.delivered_packets > 0:\n",
        "            avg_end_to_end_delay = self.total_delay / self.delivered_packets\n",
        "\n",
        "        return {\n",
        "            'avg_latency': avg_latency,\n",
        "            'avg_packet_loss': avg_packet_loss,\n",
        "            'avg_throughput': avg_throughput,\n",
        "            'avg_utilization': avg_utilization,\n",
        "            'avg_congestion': avg_congestion,\n",
        "            'packet_delivery_ratio': packet_delivery_ratio,\n",
        "            'avg_end_to_end_delay': avg_end_to_end_delay,\n",
        "            'delivered_packets': self.delivered_packets,\n",
        "            'dropped_packets': self.dropped_packets\n",
        "        }\n",
        "\n",
        "    def visualize_network(self):\n",
        "        \"\"\"Visualize the network topology\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Position nodes using spring layout\n",
        "        pos = nx.spring_layout(self.G, seed=42)\n",
        "\n",
        "        # Draw nodes\n",
        "        node_sizes = [self.G.nodes[n]['bandwidth'] * 5 for n in self.G.nodes()]\n",
        "        nx.draw_networkx_nodes(self.G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.8)\n",
        "\n",
        "        # Draw edges with width based on bandwidth and color based on congestion\n",
        "        edge_widths = [self.G[u][v]['bandwidth']/20 for u, v in self.G.edges()]\n",
        "        edge_colors = [self.G[u][v]['congestion'] for u, v in self.G.edges()]\n",
        "        nx.draw_networkx_edges(self.G, pos, width=edge_widths, edge_color=edge_colors,\n",
        "                               edge_cmap=plt.cm.RdYlGn_r, alpha=0.7)\n",
        "\n",
        "        # Add labels\n",
        "        nx.draw_networkx_labels(self.G, pos, font_size=10)\n",
        "\n",
        "        # Add edge labels (simplified to show just latency)\n",
        "        edge_labels = {(u, v): f\"{d['latency']:.1f}ms\" for u, v, d in self.G.edges(data=True)}\n",
        "        nx.draw_networkx_edge_labels(self.G, pos, edge_labels=edge_labels, font_size=8)\n",
        "\n",
        "        plt.title(\"Network Topology Visualization\")\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the network to initial state\"\"\"\n",
        "        # Reset traffic and queues\n",
        "        self.current_time = 0\n",
        "        self.packet_queue = defaultdict(list)\n",
        "        self.delivered_packets = 0\n",
        "        self.dropped_packets = 0\n",
        "        self.total_delay = 0\n",
        "\n",
        "        # Reset edge attributes\n",
        "        for u, v, data in self.G.edges(data=True):\n",
        "            data['utilization'] = random.uniform(0.1, 0.5)\n",
        "            data['congestion'] = random.uniform(0, 0.3)\n",
        "\n",
        "        # Re-initialize traffic matrix\n",
        "        self.initialize_traffic_matrix()\n",
        "\n",
        "        # Return initial observation\n",
        "        return self.get_performance_metrics()\n"
      ],
      "metadata": {
        "id": "ZIqx5kdduenc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineAgent:\n",
        "    def __init__(self, network_topology):\n",
        "        self.network = network_topology\n",
        "\n",
        "    def get_action(self, state):\n",
        "\n",
        "        # No action needed, routing is pre-calculated\n",
        "        return None  # Indicate no action to be taken\n",
        "\n",
        "    def initialize_routing_table(self):\n",
        "\n",
        "        self.routing_table = {}\n",
        "        for i in range(self.network.num_nodes):\n",
        "            self.routing_table[i] = {}\n",
        "            for j in range(self.network.num_nodes):\n",
        "                if i != j:\n",
        "                    try:\n",
        "                        # Calculate shortest path based on latency\n",
        "                        path = nx.shortest_path(\n",
        "                            self.network.G,\n",
        "                            source=i,\n",
        "                            target=j,\n",
        "                            weight=lambda u, v, d: d['latency'],\n",
        "                        )\n",
        "                        self.routing_table[i][j] = path\n",
        "                    except nx.NetworkXNoPath:\n",
        "                        self.routing_table[i][j] = None  # No path found\n",
        "\n",
        "        # Update the network's routing table\n",
        "        self.network.update_routing_table(self.routing_table)"
      ],
      "metadata": {
        "id": "AHwNld8-uhjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkEnvironment:\n",
        "    def __init__(self, network_topology, max_steps=100):\n",
        "\n",
        "        self.network = network_topology\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "        self.reset()\n",
        "\n",
        "    def action_to_routing_table(self, action, src, dst, next_hop):\n",
        "\n",
        "        # Only update if it's a valid routing decision\n",
        "        if src != dst and src != next_hop:\n",
        "            # Create a copy of the current routing table\n",
        "            new_routing_table = copy.deepcopy(self.network.routing_table)\n",
        "\n",
        "            # Check if there's a path from source to destination\n",
        "            if dst in new_routing_table[src] and new_routing_table[src][dst]:\n",
        "                # Check if next_hop is a neighbor of source\n",
        "                if self.network.G.has_edge(src, next_hop):\n",
        "                    # Find a path from next_hop to destination\n",
        "                    if dst in new_routing_table[next_hop] and new_routing_table[next_hop][dst]:\n",
        "                        # Construct new path: source -> next_hop -> ... -> destination\n",
        "                        new_path = [src] + new_routing_table[next_hop][dst]\n",
        "                        new_routing_table[src][dst] = new_path\n",
        "\n",
        "            return new_routing_table\n",
        "\n",
        "        # If invalid action, return current routing table (no change)\n",
        "        return copy.deepcopy(self.network.routing_table)\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.network.reset()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "\n",
        "        state = []\n",
        "\n",
        "        # Edge features\n",
        "        edge_list = list(self.network.G.edges(data=True))\n",
        "        for u, v, data in edge_list:\n",
        "            state.extend([\n",
        "                data['bandwidth'] / 100,  # Normalize\n",
        "                data['latency'] / 20,     # Normalize\n",
        "                data['packet_loss'],\n",
        "                data['utilization'],\n",
        "                data['congestion']\n",
        "            ])\n",
        "\n",
        "        # Node features\n",
        "        for node, data in self.network.G.nodes(data=True):\n",
        "            state.extend([\n",
        "                data['queue_size'] / 200,  # Normalize\n",
        "                data['processing_power']\n",
        "            ])\n",
        "\n",
        "        # Global metrics\n",
        "        metrics = self.network.get_performance_metrics()\n",
        "        state.extend([\n",
        "            metrics['packet_delivery_ratio'],\n",
        "            metrics['avg_end_to_end_delay'] / 100,  # Normalize\n",
        "            metrics['avg_congestion']\n",
        "        ])\n",
        "\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def step(self, action_tuple):\n",
        "\n",
        "        if action_tuple is not None:  # For RL agents\n",
        "            action_type, src, dst, next_hop = action_tuple\n",
        "            # Update routing table based on action\n",
        "            new_routing_table = self.action_to_routing_table(action_type, src, dst, next_hop)\n",
        "            self.network.update_routing_table(new_routing_table)\n",
        "\n",
        "        # Otherwise for static routing, no update needed\n",
        "\n",
        "        # Simulate the network for one time step\n",
        "        metrics_before = self.network.get_performance_metrics()\n",
        "        metrics_after = self.network.step()\n",
        "\n",
        "        # Calculate reward based on network performance metrics\n",
        "        reward = self._calculate_reward(metrics_before, metrics_after)\n",
        "\n",
        "        # Increment step counter\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check if the episode is done\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        # Additional information\n",
        "        info = metrics_after\n",
        "\n",
        "        return self._get_state(), reward, done, info\n",
        "\n",
        "    def _calculate_reward(self, metrics_before, metrics_after):\n",
        "\n",
        "        # Enhanced reward components\n",
        "        delivery_improvement = (metrics_after['packet_delivery_ratio'] - metrics_before['packet_delivery_ratio']) * 200\n",
        "        latency_improvement = (metrics_before['avg_latency'] - metrics_after['avg_latency']) / max(1, metrics_before['avg_latency']) * 5000\n",
        "        throughput_improvement = (metrics_after['avg_throughput'] - metrics_before['avg_throughput']) / 1000 * 30\n",
        "\n",
        "\n",
        "        return delivery_improvement + latency_improvement + throughput_improvement\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "\n",
        "        metrics = self.network.get_performance_metrics()\n",
        "        print(f\"Step: {self.current_step}\")\n",
        "        print(f\"Packet Delivery Ratio: {metrics['packet_delivery_ratio']:.4f}\")\n",
        "        print(f\"Average End-to-End Delay: {metrics['avg_end_to_end_delay']:.2f}ms\")\n",
        "        print(f\"Average Throughput: {metrics['avg_throughput']:.2f}Mbps\")\n",
        "        print(f\"Delivered/Dropped Packets: {metrics['delivered_packets']}/{metrics['dropped_packets']}\")\n",
        "        print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "h10nievEuj_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, num_nodes, input_dim, hidden_dim=256):\n",
        "\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        # Number of nodes in the network\n",
        "        self.num_nodes = num_nodes # Fixed for our experiments\n",
        "\n",
        "        # Network layers\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Action type (add/modify/remove route)\n",
        "        self.action_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Source node selection\n",
        "        self.src_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, self.num_nodes),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Destination node selection\n",
        "        self.dst_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, self.num_nodes),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Next hop selection\n",
        "        self.next_hop_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, self.num_nodes),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        shared_features = self.shared(x)\n",
        "\n",
        "        action_probs = self.action_head(shared_features)\n",
        "        src_probs = self.src_head(shared_features)\n",
        "        dst_probs = self.dst_head(shared_features)\n",
        "        next_hop_probs = self.next_hop_head(shared_features)\n",
        "\n",
        "        return action_probs, src_probs, dst_probs, next_hop_probs\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256):\n",
        "\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        value = self.critic(x)\n",
        "        return value\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, hidden_dim=256, lr_actor=0.0003, lr_critic=0.001,\n",
        "                 gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, epochs=10, device='cpu', num_nodes=10):\n",
        "\n",
        "        self.actor = ActorNetwork(num_nodes, state_dim, hidden_dim).to(device)\n",
        "        self.critic = CriticNetwork(state_dim, hidden_dim).to(device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
        "\n",
        "        self.num_nodes = num_nodes  # Fixed for our experiments\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "\n",
        "        # Memory buffers\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.values = []\n",
        "\n",
        "        # Training metrics\n",
        "        self.actor_losses = []\n",
        "        self.critic_losses = []\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def get_action(self, state):\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).to(self.device)\n",
        "\n",
        "        # Get action probabilities\n",
        "        with torch.no_grad():\n",
        "            action_probs, src_probs, dst_probs, next_hop_probs = self.actor(state_tensor)\n",
        "            value = self.critic(state_tensor)\n",
        "\n",
        "        # Sample action components\n",
        "        # Select one action probability and reshape\n",
        "        action_probs = action_probs.squeeze(0).view(3)  # Reshape to [3]\n",
        "\n",
        "        # Reshape src_probs, dst_probs, next_hop_probs for Categorical\n",
        "        src_probs = src_probs.squeeze(0).view(self.num_nodes)\n",
        "        dst_probs = dst_probs.squeeze(0).view(self.num_nodes)\n",
        "        next_hop_probs = next_hop_probs.squeeze(0).view(self.num_nodes)\n",
        "\n",
        "\n",
        "        action_dist = Categorical(action_probs)\n",
        "        src_dist = Categorical(src_probs)\n",
        "        dst_dist = Categorical(dst_probs)\n",
        "        next_hop_dist = Categorical(next_hop_probs)\n",
        "\n",
        "        action_type = action_dist.sample().item()\n",
        "        src = src_dist.sample().item()\n",
        "        dst = dst_dist.sample().item()\n",
        "        next_hop = next_hop_dist.sample().item()\n",
        "\n",
        "        # Calculate log probability\n",
        "        log_prob = (action_dist.log_prob(torch.tensor(action_type)) +\n",
        "                   src_dist.log_prob(torch.tensor(src)) +\n",
        "                   dst_dist.log_prob(torch.tensor(dst)) +\n",
        "                   next_hop_dist.log_prob(torch.tensor(next_hop)))\n",
        "\n",
        "        return (action_type, src, dst, next_hop), log_prob, value.item()\n",
        "\n",
        "    def remember(self, state, action, log_prob, reward, done, value):\n",
        "        \"\"\"Store experience in memory\"\"\"\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.values.append(value)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear memory buffers\"\"\"\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.values = []\n",
        "\n",
        "    def compute_gae(self, next_value):\n",
        "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
        "        values = self.values + [next_value]\n",
        "        advantages = []\n",
        "        returns = []\n",
        "        gae = 0\n",
        "\n",
        "        for i in reversed(range(len(self.rewards))):\n",
        "            delta = self.rewards[i] + self.gamma * values[i+1] * (1 - self.dones[i]) - values[i]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - self.dones[i]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            returns.insert(0, gae + values[i])\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "    def update(self, next_value):\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(np.array(self.states)).to(self.device)\n",
        "        actions = self.actions  # List of tuples\n",
        "        old_log_probs = torch.stack(self.log_probs).to(self.device)\n",
        "\n",
        "        # Compute advantages and returns\n",
        "        advantages, returns = self.compute_gae(next_value)\n",
        "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
        "        returns = torch.FloatTensor(returns).to(self.device)\n",
        "\n",
        "        # Normalize advantages\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # Extract action components\n",
        "        action_types = torch.tensor([a[0] for a in actions]).to(self.device)\n",
        "        srcs = torch.tensor([a[1] for a in actions]).to(self.device)\n",
        "        dsts = torch.tensor([a[2] for a in actions]).to(self.device)\n",
        "        next_hops = torch.tensor([a[3] for a in actions]).to(self.device)\n",
        "\n",
        "        # PPO update\n",
        "        total_actor_loss = 0\n",
        "        total_critic_loss = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            # Forward pass through actor and critic\n",
        "            action_probs, src_probs, dst_probs, next_hop_probs = self.actor(states)\n",
        "            values = self.critic(states).squeeze()\n",
        "\n",
        "            # Create distributions\n",
        "            action_dist = Categorical(action_probs)\n",
        "            src_dist = Categorical(src_probs)\n",
        "            dst_dist = Categorical(dst_probs)\n",
        "            next_hop_dist = Categorical(next_hop_probs)\n",
        "\n",
        "            # Calculate new log probabilities\n",
        "            new_log_probs = (action_dist.log_prob(action_types) +\n",
        "                            src_dist.log_prob(srcs) +\n",
        "                            dst_dist.log_prob(dsts) +\n",
        "                            next_hop_dist.log_prob(next_hops))\n",
        "\n",
        "            # Calculate entropy (for exploration)\n",
        "            entropy = (action_dist.entropy().mean() +\n",
        "                      src_dist.entropy().mean() +\n",
        "                      dst_dist.entropy().mean() +\n",
        "                      next_hop_dist.entropy().mean())\n",
        "\n",
        "            # Calculate ratio for PPO\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            # PPO surrogate losses\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
        "\n",
        "            # Actor loss (-min because we're minimizing)\n",
        "            actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
        "\n",
        "            # Critic loss (MSE)\n",
        "            critic_loss = F.mse_loss(values, returns)\n",
        "\n",
        "            # Update networks\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            total_actor_loss += actor_loss.item()\n",
        "            total_critic_loss += critic_loss.item()\n",
        "\n",
        "        # Store losses\n",
        "        avg_actor_loss = total_actor_loss / self.epochs\n",
        "        avg_critic_loss = total_critic_loss / self.epochs\n",
        "        self.actor_losses.append(avg_actor_loss)\n",
        "        self.critic_losses.append(avg_critic_loss)\n",
        "\n",
        "        # Clear memory\n",
        "        self.clear_memory()\n",
        "\n",
        "        return avg_actor_loss, avg_critic_loss\n"
      ],
      "metadata": {
        "id": "X9JwV0mfuxBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(env, agent, num_episodes=300, max_steps=100, update_frequency=20):\n",
        "\n",
        "    episode_rewards = []\n",
        "    best_reward = float('-inf')\n",
        "    best_routing = None\n",
        "\n",
        "    # For storing metrics\n",
        "    delivery_ratios = []\n",
        "    avg_latencies = []\n",
        "\n",
        "    for episode in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        # For PPO updates\n",
        "        states = []\n",
        "        actions = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        values = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action_tuple, log_prob, value = agent.get_action(state)\n",
        "\n",
        "            # Take action in environment\n",
        "            next_state, reward, done, info = env.step(action_tuple)\n",
        "\n",
        "            # Store experience\n",
        "            states.append(state)\n",
        "            actions.append(action_tuple)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            values.append(value)\n",
        "\n",
        "            # Update counters\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Track best routing configuration\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            best_routing = copy.deepcopy(env.network.routing_table)\n",
        "            print(f\"New best reward: {best_reward:.2f} at episode {episode+1}\")\n",
        "\n",
        "        # Store metrics\n",
        "        delivery_ratios.append(info['packet_delivery_ratio'])\n",
        "        avg_latencies.append(info['avg_end_to_end_delay'])\n",
        "\n",
        "        # Update agent using PPO (once per episode)\n",
        "        next_value = 0 if done else agent.get_action(next_state)[2]\n",
        "\n",
        "        # Store experiences in agent memory\n",
        "        for i in range(len(states)):\n",
        "            agent.remember(states[i], actions[i], log_probs[i], rewards[i], dones[i], values[i])\n",
        "\n",
        "        # Update if we have enough data\n",
        "        if (episode + 1) % update_frequency == 0:\n",
        "            actor_loss, critic_loss = agent.update(next_value)\n",
        "            print(f\"Episode {episode+1}: Reward={episode_reward:.2f}, Actor Loss={actor_loss:.4f}, Critic Loss={critic_loss:.4f}\")\n",
        "\n",
        "        # Store episode reward\n",
        "        episode_rewards.append(episode_reward)\n",
        "        agent.episode_rewards.append(episode_reward)\n",
        "\n",
        "    # Apply best routing at the end\n",
        "    if best_routing is not None:\n",
        "        env.network.update_routing_table(best_routing)\n",
        "\n",
        "    # Return the trained agent and metrics\n",
        "    return agent, {\n",
        "        'rewards': episode_rewards,\n",
        "        'delivery_ratios': delivery_ratios,\n",
        "        'avg_latencies': avg_latencies\n",
        "    }\n",
        "\n",
        "def evaluate_routing(env, num_iterations=50, max_steps=100):\n",
        "\n",
        "    total_rewards = []\n",
        "    delivery_ratios = []\n",
        "    avg_latencies = []\n",
        "    avg_throughputs = []\n",
        "    avg_packet_losses = []\n",
        "\n",
        "    for _ in tqdm(range(num_iterations)):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Use current routing (no action)\n",
        "            _, reward, done, info = env.step(None)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Store metrics\n",
        "        total_rewards.append(episode_reward)\n",
        "        delivery_ratios.append(info['packet_delivery_ratio'])\n",
        "        avg_latencies.append(info['avg_end_to_end_delay'])\n",
        "        avg_throughputs.append(info['avg_throughput'])\n",
        "        avg_packet_losses.append(info['avg_packet_loss'])\n",
        "\n",
        "    # Calculate averages\n",
        "    return {\n",
        "        'avg_reward': np.mean(total_rewards),\n",
        "        'std_reward': np.std(total_rewards),\n",
        "        'avg_delivery_ratio': np.mean(delivery_ratios),\n",
        "        'avg_latency': np.mean(avg_latencies),\n",
        "        'avg_throughput': np.mean(avg_throughputs),\n",
        "        'avg_packet_loss': np.mean(avg_packet_losses)\n",
        "    }\n",
        "\n",
        "def plot_training_metrics(ppo_metrics):\n",
        "    \"\"\"Plot training metrics for both agents\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Episode rewards\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(ppo_metrics['rewards'], label='Standard PPO')\n",
        "    plt.title('Episode Rewards During Training')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    # Delivery ratios\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(ppo_metrics['delivery_ratios'], label='Standard PPO')\n",
        "    plt.title('Packet Delivery Ratio During Training')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Delivery Ratio')\n",
        "    plt.legend()\n",
        "\n",
        "    # Average latencies\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(ppo_metrics['avg_latencies'], label='Standard PPO')\n",
        "    plt.title('Average End-to-End Delay During Training')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Delay (ms)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Moving average of rewards\n",
        "    plt.subplot(2, 2, 4)\n",
        "    window_size = 10\n",
        "    ppo_smooth = np.convolve(ppo_metrics['rewards'], np.ones(window_size)/window_size, mode='valid')\n",
        "    plt.plot(ppo_smooth, label='Standard PPO')\n",
        "    plt.title(f'Smoothed Rewards (Window={window_size})')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Smoothed Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_comparative_metrics(baseline_metrics, ppo_metrics):\n",
        "    \"\"\"Plot comparison of final performance metrics\"\"\"\n",
        "    metrics = ['avg_reward', 'avg_delivery_ratio', 'avg_latency', 'avg_throughput', 'avg_packet_loss']\n",
        "    labels = ['Average Reward', 'Delivery Ratio', 'Latency (ms)', 'Throughput (Mbps)', 'Packet Loss']\n",
        "\n",
        "    # Calculate improvements\n",
        "    ppo_improvement = [(ppo_metrics[m] - baseline_metrics[m]) / abs(baseline_metrics[m]) * 100 for m in metrics]\n",
        "\n",
        "    # For metrics where lower is better (latency, packet loss), invert the sign\n",
        "    for i in [2, 4]:  # Indices for latency and packet loss\n",
        "        ppo_improvement[i] = -ppo_improvement[i]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Improvement percentage\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, ppo_improvement, width, label='Standard PPO')\n",
        "\n",
        "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "    plt.xticks(x, labels)\n",
        "    plt.ylabel('Improvement over Baseline (%)')\n",
        "    plt.title('Performance Improvement Comparison')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('improvement_comparison.png')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "_sa5s22ju_fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the network and environment\n",
        "network = NetworkTopology(num_nodes=16, connectivity_prob=0.4, seed=42)\n",
        "env = NetworkEnvironment(network, max_steps=100)\n",
        "# Visualize the network\n",
        "network.visualize_network()"
      ],
      "metadata": {
        "id": "HpUN2XNKvkz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# First: Evaluate baseline (shortest path routing)\n",
        "print(\"Evaluating baseline (shortest path routing)...\")\n",
        "# Example usage\n",
        "baseline_agent = BaselineAgent(network)\n",
        "baseline_agent.initialize_routing_table()  # Initialize shortest-path routing\n",
        "baseline_metrics = evaluate_routing(env, num_iterations=50, max_steps=100)  # Evaluate\n",
        "print(\"\\nBaseline Metrics:\")\n",
        "for key, value in baseline_metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Save original routing table\n",
        "original_routing = copy.deepcopy(network.routing_table)\n",
        "\n",
        "# Train Standard PPO Agent\n",
        "print(\"\\nTraining standard PPO agent...\")\n",
        "state_dim = len(env._get_state())\n",
        "ppo_agent = PPOAgent(\n",
        "    state_dim=state_dim,\n",
        "    hidden_dim=256,\n",
        "    lr_actor=0.0003,\n",
        "    lr_critic=0.001,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_epsilon=0.2,\n",
        "    epochs=10,\n",
        "    device='cpu',\n",
        "    num_nodes=network.num_nodes\n",
        ")\n",
        "\n",
        "trained_ppo, ppo_metrics = train_agent(env, ppo_agent, num_episodes=300, max_steps=100)\n",
        "\n",
        "# Evaluate PPO agent\n",
        "print(\"\\nEvaluating standard PPO agent...\")\n",
        "ppo_eval_metrics = evaluate_routing(env)\n",
        "print(\"\\nStandard PPO Metrics:\")\n",
        "for key, value in ppo_eval_metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Save PPO routing\n",
        "ppo_routing = copy.deepcopy(network.routing_table)\n",
        "\n",
        "# Restore original routing\n",
        "network.update_routing_table(original_routing)\n",
        "\n"
      ],
      "metadata": {
        "id": "1KRKs2V5vqM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training metrics\n",
        "plot_training_metrics(ppo_metrics)\n",
        "\n",
        "# Plot comparative metrics\n",
        "plot_comparative_metrics(baseline_metrics, ppo_eval_metrics)\n",
        "\n",
        "# Print final comparison\n",
        "print(\"\\n==== FINAL COMPARISON ====\")\n",
        "print(f\"Metric                  | Baseline      | Standard PPO  )\n",
        "print(f\"------------------------|---------------|---------------|--------\")\n",
        "\n",
        "metrics_to_show = [\n",
        "    ('avg_reward', 'Average Reward'),\n",
        "    ('avg_delivery_ratio', 'Delivery Ratio'),\n",
        "    ('avg_latency', 'Latency (ms)'),\n",
        "    ('avg_throughput', 'Throughput (Mbps)'),\n",
        "    ('avg_packet_loss', 'Packet Loss')\n",
        "]\n",
        "\n",
        "for metric, label in metrics_to_show:\n",
        "    baseline = baseline_metrics[metric]\n",
        "    ppo = ppo_eval_metrics[metric]\n",
        "\n",
        "    # Calculate improvements\n",
        "    if metric in ['avg_latency', 'avg_packet_loss']:  # Lower is better\n",
        "        ppo_imp = (baseline - ppo) / baseline * 100\n",
        "    else:  # Higher is better\n",
        "        ppo_imp = (ppo - baseline) / baseline * 100\n",
        "\n",
        "    print(f\"{label:23} | {baseline:13.4f} | {ppo:13.4f} | {ppo_imp:11.2f}%\")\n",
        "\n",
        "# Visualize the network with best routing\n",
        "network.visualize_network()\n"
      ],
      "metadata": {
        "id": "msBhgs17vztN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}